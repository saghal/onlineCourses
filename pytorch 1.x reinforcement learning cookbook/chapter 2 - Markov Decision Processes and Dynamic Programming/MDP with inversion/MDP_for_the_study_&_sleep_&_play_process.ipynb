{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP for the study & sleep & play process",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "an MDP involves an agent and a decision-making process.\n",
        "\n",
        "in this session we develop an MDP and calculating the value function under the `optimal policy`.\n",
        "\n",
        "the optimal policy, in this case, is\n",
        "choosing a0 work for each step"
      ],
      "metadata": {
        "id": "G9dhwo-Mn1l7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzHKMVoJXX4h"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "rLyRghg9C5uD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### transition matrix, T(s, a, s'), consisting of the probabilities of state s transitioning to state s' with action a.\n",
        "##### s0 (study) and s1 (sleep) and s2 (play)\n",
        "##### a0 work and a1 slack"
      ],
      "metadata": {
        "id": "H7J5KxnkZsoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = torch.tensor([ \n",
        "                   [[0.8, 0.1, 0.1],\n",
        "                    [0.1, 0.6, 0.3]]\n",
        "                  ,[[0.7, 0.2, 0.1],\n",
        "                    [0.1, 0.2, 0.1]]\n",
        "                  ,[[0.6, 0.2, 0.2],\n",
        "                    [0.1, 0.4, 0.5]]\n",
        "                  ])"
      ],
      "metadata": {
        "id": "LO6drqUiexFL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R = torch.tensor([1., 0, -1.]) #reward\n",
        "gamma = 0.5\n",
        "action = 0 #optimal policy :)))))))"
      ],
      "metadata": {
        "id": "ov2LmrICfGbG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_value_matrix_inversion(gamma, trans_matrix, rewards):\n",
        "  inv = torch.inverse(torch.eye(rewards.shape[0])- gamma * trans_matrix)\n",
        "  V = torch.mm(inv, rewards.reshape(-1, 1))\n",
        "  return V"
      ],
      "metadata": {
        "id": "d_PS438Yq0W8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_matrix = T[:, action]\n",
        "V = cal_value_matrix_inversion(gamma, trans_matrix, R)\n",
        "print(\"The value function under the optimal policy is:\\n{}\".format(V))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKBQefQyrokO",
        "outputId": "d337817b-d41d-4c10-9d44-cf4aef9796fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value function under the optimal policy is:\n",
            "tensor([[ 1.6787],\n",
            "        [ 0.6260],\n",
            "        [-0.4820]])\n"
          ]
        }
      ]
    }
  ]
}