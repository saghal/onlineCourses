# -*- coding: utf-8 -*-
"""MDP for the study & sleep & play process

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rXxovVtaKjdR0qOw1Tiv-WlLZ6VA3nrl

an MDP involves an agent and a decision-making process.

in this session we develop an MDP and calculating the value function under the `optimal policy`.

the optimal policy, in this case, is
choosing a0 work for each step

## import libraries
"""

import torch

"""#### transition matrix, T(s, a, s'), consisting of the probabilities of state s transitioning to state s' with action a.
##### s0 (study) and s1 (sleep) and s2 (play)
##### a0 work and a1 slack
"""

T = torch.tensor([ 
                   [[0.8, 0.1, 0.1],
                    [0.1, 0.6, 0.3]]
                  ,[[0.7, 0.2, 0.1],
                    [0.1, 0.2, 0.1]]
                  ,[[0.6, 0.2, 0.2],
                    [0.1, 0.4, 0.5]]
                  ])

R = torch.tensor([1., 0, -1.]) #reward
gamma = 0.5
action = 0 #optimal policy :)))))))

def cal_value_matrix_inversion(gamma, trans_matrix, rewards):
  inv = torch.inverse(torch.eye(rewards.shape[0])- gamma * trans_matrix)
  V = torch.mm(inv, rewards.reshape(-1, 1))
  return V

trans_matrix = T[:, action]
V = cal_value_matrix_inversion(gamma, trans_matrix, R)
print("The value function under the optimal policy is:\n{}".format(V))