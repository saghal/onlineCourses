# -*- coding: utf-8 -*-
"""Developing the hill-climbing algorithm in cartpole env

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1exFQsBpVbSMtnPC8WVeMCu9UUaqumwj5

## Introduction
hill-climbing starts with a randomly chosen weight. But here, for
every episode, we add some noise to the weight. If the total reward improves, we update
the weight with the new one; otherwise, we keep the old weight. In this approach, the
weight is gradually improved as we progress through the episodes, instead of jumping

## import libraries
"""

import torch
import gym
import matplotlib.pyplot as plt

"""##### Define cartpole environment"""

env = gym.make('CartPole-v0')

"""##### obtain dimensions of the observation and action space"""

n_state = env.observation_space.shape[0]
n_action = env.action_space.n

"""##### run each episode with specific weight"""

def run_episode(env, weight):
  state = env.reset()
  total_reward = 0
  is_done = False
  while not is_done:
    state = torch.from_numpy(state).float()
    action = torch.argmax(torch.matmul(state, weight))
    state, reward, is_done, _ = env.step(action.item())
    total_reward += reward
  return total_reward

n_episode = 1000
best_total_reward = 0
best_weight = torch.rand(n_state, n_action)
total_rewards = []
noise_scale = 0.01

for episode in range(n_episode):
  weight = best_weight + noise_scale * torch.rand(n_state, n_action)
  total_reward = run_episode(env, weight)
  print('Episode {}: {}'.format(episode+1, total_reward))
  if total_reward > best_total_reward:
    best_weight = weight
    best_total_reward = total_reward
    noise_scale = max(noise_scale / 2, 1e-4) #noise lower bound is 0.0001
  else:
    noise_scale = min(noise_scale*2 , 2) #noise upper bound is 2
  total_rewards.append(total_reward)

print("average reward in hill-climbing", sum(total_rewards)/n_episode)

"""##### now, evalute best weight which obtained in hill-climbing."""

n_episode_eval = 100
total_rewards_eval = []
for episode in range(n_episode_eval):
  total_reward = run_episode(env, best_weight)
  print('Episode {}: {}'.format(episode+1, total_reward))
  total_rewards_eval.append(total_reward)

print('Average total reward over {} episode: {}'.format(n_episode_eval, sum(total_rewards_eval) / n_episode_eval))

plt.plot(total_rewards)
plt.xlabel('Episode')
plt.ylabel('Reward')
plt.show()